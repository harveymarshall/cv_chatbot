{
  "name": "Harvey Marshall",
  "email": "marshall.harvey@outlook.com",
  "summary": "I currently work as a Data Engineer for PetsAtHome. I am predominantly working to build out a new Data Lakehouse utilising Microsoft Fabric to facilitate real-time analytics and Machine Learning Platforms. In My spare time I am partaking in the IBM AI Engineering Cerftification in a hope to transfer into an AI engineer role. My ability to adapt and learn is something I believe makes me stand out from the crowd. I have a very positive attitude and seek to create a good culture among all those I work with. One thing I have found in common with all my previous and current roles is that the best place to learn is from those around you. It is always better to ask questions and learn from those you work with. Among my strongest skills is my customer service. All the roles I have held have a strong emphasis on people; this has allowed me to solidify my people management and interaction skills. I am confident under pressure and have experience working towards deadlines and working within a fast-paced environment.\nI am not afraid to ask questions and that is something that my current role requires of me on a daily basis. I believe that asking questions is a compulsory skill for any job. It's better to ask a question than struggle with no answer or guidance.\nA big thing for me and my workplace is the culture. I want to work somewhere that puts the people, whether that is customers or employees at the forefront of everything they do.",
  "experience": [
    {
      "company": "PetsAtHome",
      "role": "Data Engineer",
      "years": "2024-Current",
      "summary": "I work within the MLOps squad at pets to build out a new Data Lakehouse platform using new Microsoft Fabric tooling, this platform will be used to facilitate Operational analytics and Machine Learning Platforms. I am also looking at ways we can utilise AI within our workflow using AI agents to increase productivity. I also work within the Analytics Platform and Data Engineering team at Pets At Home. Where the past few months I have been the single colleague working on designing solutions to ingest data from varying sources into our Data lake on the Google Cloud Platform. The main subject of my work has been to integrate Azure Event services such as Service Bus and Event Hub. I have also created a custom application allowing non-technical users to generate Apache airflow pipelines just using YAML files. Alongside other work I have used docker containers to build code used within apache airflow to ingest and sink various data to a variety of sources and destinations. I also helped to build and manage a system for deploying dbt models within this new DAG infrastructure and used the DBT tooling to help build out our data lake using BigQuery External tables defined on top of Google Cloud Storage buckets. As part of this I completed the DBT fundamentals course to ensure my skills in DBT were adequate."
    }
  ]
}
